{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "my = Mystem()\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Сложности в тексте:\n",
    "- имена собственные\n",
    "- аббревиатуры (МГУ им. М.В.Ломоносова)\n",
    "- соркащения (ср., и т.д.)\n",
    "- слова через дефис (в марте-апреле, IT-технологии)\n",
    "- составные предлоги и союзы (в связи с, как... так и)\n",
    "- вводные слова (например, наконец, счастливым образом)\n",
    "- наречия на основе словосочетаний (как минимум, как максимум)\n",
    "- нестандартная пунктуация (')\n",
    "- модальные предикаты, союзные слова, частица \"ли\" - все они распознаются парсерами как попало\n",
    "'''\n",
    "\n",
    "#Текст без разметки\n",
    "with open('forum.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Форум', 'NOUN'], ['«', 'PUNCT'], ['Оценка', 'NOUN'], ['методов', 'NOUN'], ['автоматического', 'ADJ'], ['анализа', 'NOUN'], ['текста', 'NOUN'], ['»', 'PUNCT'], ['стартовал', 'VERB'], ['в', 'ADP']]\n"
     ]
    }
   ],
   "source": [
    "'''Для разметки выбран тегсет UPOS плюс тег PARENTH для вводных слов. \n",
    "Выбираю его, так как он основан на общепринятых UD и адаптирован для русского материала.\n",
    "Но тега для вводных слов там нет...'''\n",
    "\n",
    "#Текст с ручной разметкой, формат TSV\n",
    "table = []\n",
    "\n",
    "with open('forum_table.txt', 'r', encoding='utf-8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsvreader:\n",
    "        table.append([row[0], row[1]])\n",
    "    f.close()\n",
    "    \n",
    "print(table[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3 балла) Потом вам будет нужно взять три POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Форум', 'S'], ['Оценка', 'S'], ['методов', 'S'], ['автоматического', 'A'], ['анализа', 'S'], ['текста', 'S'], ['стартовал', 'V'], ['в', 'PR'], ['феврале', 'S'], ['года', 'S']]\n"
     ]
    }
   ],
   "source": [
    "#Mystem\n",
    "mystem_tags = []\n",
    "mystem_an = my.analyze(text)\n",
    "\n",
    "for word in mystem_an:\n",
    "    if 'analysis' in word:\n",
    "        if word['analysis'] == []:\n",
    "            mystem_tags.append((word['text'], 'X'))\n",
    "        else:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            mystem_tags.append([word['text'], pos])\n",
    "            \n",
    "print(mystem_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['форум', 'NOUN'], ['«', 'PNCT'], ['оценка', 'NOUN'], ['методов', 'NOUN'], ['автоматического', 'ADJF'], ['анализа', 'NOUN'], ['текста', 'NOUN'], ['»', 'PNCT'], ['стартовал', 'VERB'], ['в', 'PREP']]\n"
     ]
    }
   ],
   "source": [
    "#Pymorphy\n",
    "pymorphy_tags = []\n",
    "\n",
    "for token in simple_word_tokenize(text):\n",
    "    pymorphy_an = morph.parse(token)\n",
    "    pos = str(pymorphy_an[0].tag)\n",
    "    pymorphy_tags.append([pymorphy_an[0].word, pos[:4]])\n",
    "    \n",
    "print(pymorphy_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Форум', 'NOUN'], ['«', 'PUNCT'], ['Оценка', 'NOUN'], ['методов', 'NOUN'], ['автоматического', 'ADJ'], ['анализа', 'NOUN'], ['текста', 'NOUN'], ['»', 'PUNCT'], ['стартовал', 'VERB'], ['в', 'ADP']]\n"
     ]
    }
   ],
   "source": [
    "#Natasha\n",
    "natasha_tags = []\n",
    "\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "for token in doc.tokens:\n",
    "    natasha_tags.append([token.text, token.pos])\n",
    "\n",
    "print(natasha_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Форум', 'NOUN'], ['Оценка', 'NOUN'], ['методов', 'NOUN'], ['автоматического', 'ADJ'], ['анализа', 'NOUN'], ['текста', 'NOUN'], ['стартовал', 'VERB'], ['в', 'ADP'], ['феврале', 'NOUN'], ['года', 'NOUN']]\n"
     ]
    }
   ],
   "source": [
    "mystem_dict = {\n",
    "    'S': 'NOUN',\n",
    "    'A': 'ADJ',\n",
    "    'V': 'VERB',\n",
    "    'PR': 'ADP',\n",
    "    'CONJ': 'CCONJ',\n",
    "    'SPRO': 'PRON',\n",
    "    'ADVPRO': 'ADV',\n",
    "    'APRO': 'DET',\n",
    "    'X': 'X',\n",
    "    'PART': 'PART',\n",
    "    'ANUM': 'ADJ',\n",
    "    'ADV': 'ADV'\n",
    "}\n",
    "\n",
    "new_mystem_tags = []\n",
    "\n",
    "for tag in mystem_tags:\n",
    "    new_mystem_tags.append([tag[0], mystem_dict[tag[1]]])\n",
    "    \n",
    "print(new_mystem_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['форум', 'NOUN'], ['«', 'PUNCT'], ['оценка', 'NOUN'], ['методов', 'NOUN'], ['автоматического', 'ADJ'], ['анализа', 'NOUN'], ['текста', 'NOUN'], ['»', 'PUNCT'], ['стартовал', 'VERB'], ['в', 'ADP']]\n"
     ]
    }
   ],
   "source": [
    "pymorphy_dict = {\n",
    "    'NOUN': 'NOUN',\n",
    "    'VERB': 'VERB',\n",
    "    'PNCT': 'PUNCT',\n",
    "    'INFN': 'VERB',\n",
    "    'ADJF': 'ADJ',\n",
    "    'PREP': 'ADP',\n",
    "    'CONJ': 'CCONJ',\n",
    "    'LATN': 'X',\n",
    "    'ADVB': 'ADV',\n",
    "    'PRTF': 'VERB',\n",
    "    'NUMB': 'NUM',\n",
    "    'NPRO': 'DET',\n",
    "    'PRED': 'VERB',\n",
    "    'PRCL': 'PART',\n",
    "    'ADJS': 'ADJ'\n",
    "}\n",
    "\n",
    "new_pymorphy_tags = []\n",
    "\n",
    "for tag in pymorphy_tags:\n",
    "    new_pymorphy_tags.append([tag[0], pymorphy_dict[tag[1]]])\n",
    "    \n",
    "print(new_pymorphy_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 283 281 266\n"
     ]
    }
   ],
   "source": [
    "'''Чтобы измерить accurcay, придётся справиться с тем, что кол-во токенов получилось разным. \n",
    "Mystem, например, игнорирует всю пунктуацию.'''\n",
    "print(len(new_mystem_tags), len(new_pymorphy_tags), len(natasha_tags), len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "\n",
    "for token in table:\n",
    "    y_true.append(token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mystem = []\n",
    "\n",
    "mystem_copy = copy.deepcopy(new_mystem_tags)\n",
    "\n",
    "for true_token in table:\n",
    "    for pred_token in mystem_copy:\n",
    "        if true_token[0] == pred_token[0]:\n",
    "            y_mystem.append(pred_token[1])\n",
    "            mystem_copy.remove(pred_token)\n",
    "            break\n",
    "        elif pred_token == mystem_copy[-2]: #элемент [-1] - это '.'\n",
    "            y_mystem.append('N/A')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6954887218045113"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pymorphy = []\n",
    "\n",
    "pymorphy_copy = copy.deepcopy(new_pymorphy_tags)\n",
    "\n",
    "for true_token in table:\n",
    "    for pred_token in pymorphy_copy:\n",
    "        if true_token[0].lower() == pred_token[0]:\n",
    "            y_pymorphy.append(pred_token[1])\n",
    "            pymorphy_copy.remove(pred_token)\n",
    "            break\n",
    "        elif pred_token == pymorphy_copy[-2]: \n",
    "            y_pymorphy.append('N/A')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345864661654135"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pymorphy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_natasha = []\n",
    "\n",
    "natasha_copy = copy.deepcopy(natasha_tags)\n",
    "\n",
    "for true_token in table:\n",
    "    for pred_token in natasha_copy:\n",
    "        if true_token[0] == pred_token[0]:\n",
    "            y_natasha.append(pred_token[1])\n",
    "            natasha_copy.remove(pred_token)\n",
    "            break\n",
    "        elif pred_token == natasha_copy[-2]: \n",
    "            y_natasha.append('N/A')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308270676691729"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_natasha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#у пайморфи и наташи получился примерно одинаковый результат. возьму наташу, чтобы не путаться в pos-тегах\n",
    "\n",
    "def chuncker(text):\n",
    "    natasha_tags = []\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        natasha_tags.append([token.text, token.pos])\n",
    "\n",
    "    adp_noun = []\n",
    "    for i in range(len(natasha_tags)-1):\n",
    "        if natasha_tags[i][1] == 'ADP':\n",
    "            if natasha_tags[i+1][1] == 'NOUN':\n",
    "                adp_noun.append(natasha_tags[i][0] + ' ' + natasha_tags[i+1][0])\n",
    "                \n",
    "    adj_noun = []\n",
    "    for i in range(len(natasha_tags)-1):\n",
    "        if natasha_tags[i][1] == 'ADJ':\n",
    "            if natasha_tags[i+1][1] == 'NOUN':\n",
    "                adj_noun.append(natasha_tags[i][0] + ' ' + natasha_tags[i+1][0])\n",
    "                \n",
    "    before_dot = []\n",
    "    for i in range(len(natasha_tags)-1):\n",
    "        if natasha_tags[i][0] == '.':\n",
    "            before_dot.append(natasha_tags[i-1][0] + natasha_tags[i][0])\n",
    "            \n",
    "    return adp_noun, adj_noun, before_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['в феврале',\n",
       "  'в марте-апреле',\n",
       "  'на конференции',\n",
       "  'на существование',\n",
       "  'для построения',\n",
       "  'в мире',\n",
       "  'В связи',\n",
       "  'в названии',\n",
       "  'при анализе'],\n",
       " ['автоматического анализа',\n",
       "  '2010 года',\n",
       "  'первого года',\n",
       "  'морфологические парсеры',\n",
       "  'русского языка',\n",
       "  'Тестовый запуск',\n",
       "  'очную встречу',\n",
       "  'постоянными участниками',\n",
       "  'русского языка',\n",
       "  'новые процессоры',\n",
       "  'лингвистические ресурсы',\n",
       "  'лингвистических систем',\n",
       "  'разных этапах',\n",
       "  'большой популярностью',\n",
       "  'некоммерческие семинары',\n",
       "  'сравнительной оценке',\n",
       "  'русскоязычных ресурсов',\n",
       "  'Счастливым образом',\n",
       "  '2010 года',\n",
       "  'образовательную составляющую',\n",
       "  'финального отчета',\n",
       "  'активное участие',\n",
       "  'прикладной лингвистики',\n",
       "  'филологического факультета',\n",
       "  'слабые стороны',\n",
       "  'внутренней документации',\n",
       "  'морфологический парсер',\n",
       "  'текстового слова'],\n",
       " ['языка.', '».', 'ср.', 'др.', 'ресурсов.', 'им.', 'М.', 'В.', 'т.', 'д.'])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chuncker(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2-3 балла) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без (это бонусный одиннадцатый балл)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ADJ + цена \n",
    "2. ADJ + качество\n",
    "3. не + VERB\n",
    "\n",
    "1 и 2 помогают отличить \"низкую цену\" и \"высокое качество\" от \"высокой цены\" и \"низкого качества\" - без сущ. значение прил. не даст нам определить окраску отзыва.\n",
    "3 поможет отличить \"не понравился\", \"не оценил\", \"не стоит\" от \"не подвёл\", \"не разочаровал\" и т.п."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
